\newcommand{\Exp}{\mathbb{E}}
\newcommand{\Nat}{\mathbb{N}}

\begin{definition}
    Expected Value
\end{definition}
Outcomes of an experiment or random process are real numbers $a_1, \dots a_n$ with probabilities $p_1, \dots, p_n$
\[ 
    \sum_{k=1}^{n} a_kp_k = a_1p_1 + \dots + a_np_n
\]

\begin{example}
Lottery example: 500,000 people pay \$5 with winners 1 x \$1,000,000, 10 x \$1,000, 1000 x \$500, 10,000 x \$10. What's the expected value of the ticket? \\
$p_k = \frac{1}{500,000}$ for each $k = 1, \dots, 500000$ $p_k$ is the probability of each outcome occuring. $a_i$ is the net gain for a ticket $a_i$ where $a_1 = 999,995$ the net gain for winning minus the \$5 cost. 
2nd prize $a_2, \dots, a_{11} = 995$, 3rd = $a_{12}, \dots, a_{1011} = 495$ 4th = $a_{1011}, \dots, a_{1012} = 10$, 5th = $a_{1012}, \dots, a_{11011} = 10$, remainder $a_{11011}, \dots, a_{500000} = -5$
Expected value of a ticket is 
\[
    \sum_{k=1}^{500000} a_kp_k. \; Given \; p_k = \frac{1}{500000}, then \frac{1}{500000} \cdot \sum_{k=1}^{500000} a_k
\]
\[
    \frac{1}{500000}(999,995 + 10 \cdot 995 + 100 \cdot 495 + 10000 \cdot 5 + (-5) \cdot 488989) = -1.78
\]
A person who plays this lottery will on average lose 1.78 per ticket!
\end{example}


\begin{example}
    How many consecutive pairs of the same suit are expected in a deck of cards?
    \begin{enumerate}
        \item Define Indicator variables: Deck of cards = 52, 13 each suit. Let $X_i = 1$ if $i$ and $i+1$ are same suit, 0 otherwise.
        \item Calculate $P(X_i = 1)$. First $P$ = $\frac{1}{52}$, Second card matches first = $\frac{12}{51}$ for each $i$.
        \item Linearity of expectation: Total no. pairs
    \end{enumerate}

\end{example}

















Experiment
Shuffle a deck of cards, go through in order. How many times do 2 consecutive cards have the same suit?

\subsection{Linearity of expectation}
...The sum of each little thing
\[\Exp [ X + Y ] = \Exp[X] + \Exp[Y]\]
No assumption of independence or anything. Surprisingly useful. 

Prove
Expectation of selecting a card of type 1
1/13

sum of all expecations 
Xi depends on 2 cards. What's Pr

Probability of 
52 C 13
E[Xi] = Pr[Xi = 1] 


Note: difference between Expectation and Probability.
Probability = the likelihood of the event e.g. selecting 2 consecutive suit cards from a deck of 52
Expectation = the average outcome. Multiply each outcome by it's probability. 

\todo{need to do this}

\todo{discuss difference between}
Monte Carlo, Las Vegas
Running time
Output quality



Question
I have an array with n = 100
index of an even number
what is time complexity of getting even numbers
Theta n

- why isn't this constant? because you can create an algorithm that only selects 

"on expectation", the las Vegas

expected time 
For all expectation [Ta] = sum from infiintiy i = 1, i Pr [takes i attempts to find an even number]
\[
    \Exp[T_a] = \sum_{i=1}^{\infty} i \cdot \Pr[\text{takes $i$ attempts to find an even number}]
\]
\[
    = \sum_{i=1}^{\infty} \frac{i}{2^i} = O(1)    
\]


Why Randomization?


Faster, Simpler Algo'S
- miller rabin, it's a monte carlo algo. 
Runs in $O tilde n^2$



Algos
Quicksort
Expected running time 
Is it Las Vegas or Monte Carlo?
It's always going to return the sorted array, so it's Las Vegas.
Proof:
T(n) = Expectations[runtime on array size n]
T(n) = E[T(|A1|)] + E[T(|A2|)] + O(n)
We know |A1| + |A2| = n-1 |Why n-1? 

Expected time analysis vs worst case.
expected time analysis: for randomized algorithms
average time analysis: using input from a known probability distribution
amortized analysis: reusing algorithm on a sequence of inputs, and look at the worst-case sequence of input for the algorithm divided by the length of the sequence








\section*{Tutorial 1}
\subsubsection*{Expectation, Discrete Random Variable}
Change to definitions here: https://proofwiki.org/wiki/Definition:Expectation
\begin{itemize}
    \item $E[X]$ = expectation of random variable $X$
    \item $X$ is a discrete random variable having probability mass function $p(x)$, then $E[X]$ is defined by 
    \[
        E[X] = \sum_{x:p(x)>0}^{} xp(x)
    \]
    \item Probability Mass Function (PMF) $p(x)$ gives $\Pr$ that a discrete random variable $X$ is equal to some value $x$
    \item PMF: $p(x) \geqq 0$ for all $x$, $\sum_{}^{}p(x) = 1$ the sum of probabilities over all possible values
\end{itemize}
\subsubsection*{Expectation, Continuous Random Variable}
\begin{itemize}
    \item $X$ is a continuous random variable having probability mass function $f(x)$, then $E[X]$ is defined by 
    \[
        E[X] = \int_{x:p(x)>0}^{} xf(x)d(x)
    \]
    \item PDF = probability density function
\end{itemize}

\subsubsection*{Variance}
\begin{itemize}
    \item Def: average of the squared differences from the mean
           \[
                X: Var(X) = \Exp[(X-\Exp[X]^2)] \; \text{By Linearity =} \Exp[X^2] - (E[X])^2
            \]      
    \item $Var(aX + b) = a^2Var(X)$ for constants $a$ and $b$
    \item For independent random variables: $Var(X + Y) = Var(X) + Var(Y)$
    \item Indicates how much a data point deviates from the mean, low variance = close to mean, high variance = far from mean, wider range
    \item Example: population $\sigma^2 = \sum(X - \mu)^2 / N.$ Sample: $s^2 = \sum  \tfrac{(X - X)^2}{n-1}$.
    \item $\sigma^2 / s^2$ is the variance, $X$ is each value in a data set, $\mu or X$ is the mean, $N or n$ is number of data points 
    \item Expressed as squared units of the original data, sometimes confusing
    \item Standard deviation is the square root of variance, in the same units of the original data
    \item 
\end{itemize}

\subsubsection*{Complement Rule}
For event $A$, $\Pr(A) + \Pr(\text{not } A) = 1 \equiv \Pr(\text{not } A) = 1 - P(A)$

\subsubsection*{Expectation of Binomial Distribution}
$X$ is a discrete random variable with binomial distribution parameters $n, p$ for $n \in \Nat$ and $0 \leq p \leq 1$ then $\Exp[X] = np$
Proof: $https://proofwiki.org/wiki/Expectation_of_Binomial_Distribution$

\section*{Tutorial 1}
Problem 1
Consider a deck of 4$n$ cards with 'S', 'H', 'D', 'C', after shuffled randomly, what's the expected number of consecutive pairs of the same suit.
\begin{enumerate}
    \item Define Indicator Variable $X_i$ for each iteration required. We have $4n-1$ because $4n$ cards, minus 1 match because the last card can't be matched with the null pointer next door. Let $S$ = shuffled card
    \[
    X_i =
    \begin{cases} 
    1 & \text{if $S_i = S_{i + 1}$ } \\
    0 & \text{else}
    \end{cases}
    \]
    \item Sum the number of consecutive pairs found, i.e., sum all 1 cases
    \[
        X = \sum_{i=1}^{4n-1} X_i \; \text{we need to find }E[X] = E[\sum_{i=1}^{4n-1} X_i] \;
    \]
    By linearity
    \[
         = \;  \sum_{i=1}^{4n-1} E[X_i] \; = \;  \sum_{i=1}^{4n-1} \frac{\text{no. cards in a suit}}{\text{no. cards in a deck}}
    \]
    We need to find $E[X_i]$ for each $i$ = $\Pr$ that $S_i = S_{i + 1}$:
    \[
        \Pr(X_i = 1) = \frac{n-1}{4n-1} \; = E[X_i] = \frac{n-1}{4n-1}\; 
    \]
\end{enumerate}

Problem 2
Similar

Problem 3
Similar
Variance part explained here https://claude.ai/chat/9210ca3a-e032-4137-80b1-455acfe9835a

Problem 4
explained here https://claude.ai/chat/fc539a53-9b45-4e59-9638-fcd3e8532612 

Problem 5




\section*{Quiz 0}
Q1: Handshaking Lemma states if $G = (V,E)$ is an undirected graph, $\sum_{v \in V}^{} deg v$ is equal to: $2|E|$. \\
Recall E0--v--0E 

Q2: Linearity of expectation means if $X, Y$ are 2 arbitrary random variables and $a, b$ are 2 arbitrary random numbers then $\Exp [aX + bY] = \Exp[aX] + \Exp[bY] = a\Exp[X] + b\Exp[Y]$ if:
\begin{enumerate}
    \item as long as both expectations are defined
    \item only if X, Y are independent
    \item only if a, b are positive
    \item only if X, Y are uncorrelated
\end{enumerate}

\newenvironment{QandA}{\begin{enumerate}[label=\bfseries\alph*.,leftmargin=2em,rightmargin=2em]\bfseries}
    {\end{enumerate}}
\newenvironment{answered}{\par\normalfont}{}

\section*{Quiz 1}
\begin{enumerate}
    \item if $X$ is the indicator variable of event $E$, then $\Exp[X]$ is...  \\
    $\Pr[E]$ because by def. $\Exp[X]$ is the sum of the probability of each event happening times its outcome. An indicator variable is related to 1 single event.
    
    \item The $\Omega(n\log n)$ worst-case lower bound for comparison based sorting algorithms doesn't apply to randomised algorithms. \\
    False. Why? \\

    \item What type of randomized algorithm is randomized QuickSort? \\
    Las Vegas, will produce the correct result with randomized running time. Monte Carlo is 0.99 percent correct with fixed running time. \\
    Randomized QuickSort fits Las Vegas because it optimizes for correctness not speed, running time varies.
    QuickSort needs to arrange elements in order and needs correctness (the ordering can't be wrong) \\
    Efficiency = expected O(n log n), worst case $O^2$ 

    \item if a random variable has well-defined variance and expectation, then \\
    $Var[X] \leq \Exp[X^2]$ as per definition of $X$ with finite variance
    Question: What's the difference between $\Exp[X^2] and \Exp[X]^2$ - something to do with constants?
    X is a random variable not a constant. Exp[X] is a constant.

    \item The expected number of comparisons by Randomized QuickSort is at most \\
    $\mathcal{O}(n \log n)$ - why?

    \item For a given input $x$ a randomied algo $A$ always gives different answers when given different random strings \\
    False: (I guessed True). Why? \\
    
    What is the purpose of randomization!? Many cases it's to improve average case performance or avoid worst-case performance, not to produce varying output.
    Often there is consistent output for a given input. Randomness is used not necessarily as encryption but to enable the algorithm to find the solution faster.
    E.g. Randomized Quicksort: the final sorted array is the same but choice of pivots is randomized
    

    \item We have a much faster polynomial-time randomised algorithm than the best known deterministic algorithm for:\\
    integer primality testing: Miller Rabin or the other one Clément mentioned that's $n^6$
    
    \item Suppose I'm given a randommized algorithm A which takes integer $n$ as input and outputs a random not necessarily uniform English sentence of exactly $n$ words. If I run the algorithm twice with input $n$ with the same random bits, will I get the same sentence? \\
    Yes because the source of randomness is the same.

    randomString = "10110101"
    fn A(n) -> sentence {

    }

    \item What is a possible limitation to keep in mind when using randomised algorithms?
    They assume access to good randomness (not they're harder to analyse, my answer)
    
    \item If the expected running time of an algorithm of input size $n$ is $\mathcal{O}(1)$ then worst-case running time is always $\mathcal{O}(1)$ \\
    False! it would be the proper worse case 


\end{enumerate}